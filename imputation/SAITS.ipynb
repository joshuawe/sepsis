{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAITS Imputation with PyPOTS\n",
    "\n",
    "> Notebook adapted in parts from [PyPOTS](https://github.com/WenjieDu/PyPOTS/).  <center>  \n",
    "  <img src=\"https://raw.githubusercontent.com/WenjieDu/PyPOTS/main/docs/figs/PyPOTS%20logo.svg?sanitize=true\" width=\"150\" height=\"150\" />\n",
    "  </center>\n",
    "\n",
    "<details>\n",
    "        <summary> Using your own dataset </summary>\n",
    "<div>   \n",
    "```\n",
    "{\n",
    "# Install PyPOTS first: pip install pypots  \n",
    "from pypots.data import mcar, fill_nan_with_mask  \n",
    "from pypots.imputation import SAITS  \n",
    "from pypots.utils.metrics import cal_mae  \n",
    "\n",
    "# ‚ùóÔ∏èüëÄ Preprocess your data here to generate X, which should be a NumPy array\n",
    "# X.shape should be [n_samples, T, D]\n",
    "\n",
    "# hold out some observed values as ground truth\n",
    "X_intact, X, missing_mask, indicating_mask = mcar(X, 0.1)\n",
    "X = fill_nan_with_mask(X, missing_mask)\n",
    "\n",
    "saits_base = SAITS(n_layers=2, d_model=256, d_inner=128, n_head=4, d_k=64, d_v=64, dropout=0.1, epochs=100)\n",
    "saits_base.fit(X) # here use the whole set. You can also split X into train/val/test sets.\n",
    "imputation = saits_base.impute(X)\n",
    "mae = cal_mae(imputation, X_intact, indicating_mask)  # calculate mean absolute error on imputation\n",
    "}```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "\n",
    "```\n",
    "# Install PyPOTS first: pip install pypots  \n",
    "from pypots.data import mcar, fill_nan_with_mask  \n",
    "from pypots.imputation import SAITS  \n",
    "from pypots.utils.metrics import cal_mae  \n",
    "\n",
    "# ‚ùóÔ∏èüëÄ Preprocess your data here to generate X, which should be a NumPy array\n",
    "# X.shape should be [n_samples, T, D]\n",
    "\n",
    "# hold out some observed values as ground truth\n",
    "X_intact, X, missing_mask, indicating_mask = mcar(X, 0.1)\n",
    "X = fill_nan_with_mask(X, missing_mask)\n",
    "\n",
    "saits_base = SAITS(n_layers=2, d_model=256, d_inner=128, n_head=4, d_k=64, d_v=64, dropout=0.1, epochs=100)\n",
    "saits_base.fit(X) # here use the whole set. You can also split X into train/val/test sets.\n",
    "imputation = saits_base.impute(X)\n",
    "mae = cal_mae(imputation, X_intact, indicating_mask)  # calculate mean absolute error on imputation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyPOTS first: pip install pypots\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from pypots.data import load_specific_dataset, mcar, masked_fill\n",
    "from pypots.imputation import SAITS, BRITS\n",
    "from pypots.utils.metrics import cal_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysionNET Dataset\n",
    "## Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing. Tedious, but PyPOTS can help. ü§ì\n",
    "data = load_specific_dataset('physionet_2012')  # For datasets in PyPOTS database, PyPOTS will automatically download and extract it.\n",
    "X = data['X']\n",
    "num_samples = len(X['RecordID'].unique())\n",
    "\n",
    "print('num samples: ', num_samples)\n",
    "print(X)\n",
    "X = X.drop('RecordID', axis = 1)\n",
    "X = StandardScaler().fit_transform(X.to_numpy())\n",
    "X = X.reshape(num_samples, 48, -1)\n",
    "X_intact, X, missing_mask, indicating_mask = mcar(X, 0.1) # hold out 10% observed values as ground truth\n",
    "X = masked_fill(X, 1 - missing_mask, np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SAITS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No given device, using default device: cuda:0\n",
      "Model initialized successfully. Number of the trainable parameters: 1378358\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Model training. This is PyPOTS showtime. üí™\u001b[39;00m\n\u001b[1;32m      2\u001b[0m saits \u001b[39m=\u001b[39m SAITS(n_steps\u001b[39m=\u001b[39m\u001b[39m48\u001b[39m, n_features\u001b[39m=\u001b[39m\u001b[39m37\u001b[39m, n_layers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, d_model\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, d_inner\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, n_head\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, d_k\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, d_v\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, dropout\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m saits\u001b[39m.\u001b[39mfit(X)  \u001b[39m# train the model. Here I use the whole dataset as the training set, because ground truth is not visible to the model.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m imputation \u001b[39m=\u001b[39m saits\u001b[39m.\u001b[39mimpute(X)  \u001b[39m# impute the originally-missing values and artificially-missing values\u001b[39;00m\n\u001b[1;32m      5\u001b[0m mae \u001b[39m=\u001b[39m cal_mae(imputation, X_intact, indicating_mask)  \u001b[39m# calculate mean absolute error on the ground truth (artificially-missing values)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Model training. This is PyPOTS showtime. üí™\n",
    "saits = SAITS(n_steps=48, n_features=37, n_layers=2, d_model=256, d_inner=128, n_head=4, d_k=64, d_v=64, dropout=0.1, epochs=10)\n",
    "saits.fit(X)  # train the model. Here I use the whole dataset as the training set, because ground truth is not visible to the model.\n",
    "imputation = saits.impute(X)  # impute the originally-missing values and artificially-missing values\n",
    "mae = cal_mae(imputation, X_intact, indicating_mask)  # calculate mean absolute error on the ground truth (artificially-missing values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy dataset\n",
    "\n",
    "The data set here was generated by using a script from the **AFA** repository from *Henrik v. Kleist*.\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples:  10\n"
     ]
    }
   ],
   "source": [
    "path_toydataset = '/home2/joshua.wendland/Documents/sepsis/toy_dataset/synthetic_ts_1/synthetic_ts_test_data_eav.csv.gz'\n",
    "\n",
    "df = pd.read_csv(path_toydataset, compression=None)\n",
    "df = df.sort_values(by=['id', 'time'], ascending=True, ignore_index=True)  # time was not sorted\n",
    "#df = df.loc[df['id'] == 'id_90']\n",
    "num_samples = len(df['id'].unique())\n",
    "print('num samples: ', num_samples)\n",
    "\n",
    "X = df.drop('id', axis = 1)\n",
    "X = StandardScaler().fit_transform(X.to_numpy())\n",
    "X[:,0] = df['time'].to_numpy() # time should not be normalized\n",
    "X = X.reshape(num_samples, 50, -1)\n",
    "X_intact, X, missing_mask, indicating_mask = mcar(X, 0.5) # hold out 10% observed values as ground truth\n",
    "X = masked_fill(X, 1 - missing_mask, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "### SAITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No given device, using default device: cuda:0\n",
      "Model initialized successfully. Number of the trainable parameters: 1326476\n",
      "epoch 0: training loss 7.7477\n",
      "epoch 1: training loss 8.0805\n",
      "epoch 2: training loss 7.5872\n",
      "epoch 3: training loss 8.1251\n",
      "epoch 4: training loss 5.4035\n",
      "epoch 5: training loss 4.5630\n",
      "epoch 6: training loss 6.8829\n",
      "epoch 7: training loss 4.7526\n",
      "epoch 8: training loss 5.6988\n",
      "epoch 9: training loss 4.8192\n",
      "epoch 10: training loss 4.7027\n",
      "epoch 11: training loss 6.8866\n",
      "epoch 12: training loss 5.9552\n",
      "epoch 13: training loss 5.7504\n",
      "epoch 14: training loss 7.0227\n",
      "epoch 15: training loss 5.7542\n",
      "epoch 16: training loss 5.0919\n",
      "epoch 17: training loss 5.5334\n",
      "epoch 18: training loss 4.2338\n",
      "epoch 19: training loss 4.1536\n",
      "epoch 20: training loss 4.4698\n",
      "epoch 21: training loss 4.1121\n",
      "epoch 22: training loss 3.9592\n",
      "epoch 23: training loss 3.4621\n",
      "epoch 24: training loss 3.8195\n",
      "epoch 25: training loss 3.7491\n",
      "epoch 26: training loss 3.6877\n",
      "epoch 27: training loss 3.6544\n",
      "epoch 28: training loss 4.2555\n",
      "epoch 29: training loss 3.3267\n",
      "epoch 30: training loss 3.3089\n",
      "epoch 31: training loss 3.7447\n",
      "epoch 32: training loss 3.3728\n",
      "epoch 33: training loss 3.1137\n",
      "epoch 34: training loss 3.2480\n",
      "epoch 35: training loss 3.4044\n",
      "epoch 36: training loss 3.3422\n",
      "epoch 37: training loss 3.5002\n",
      "epoch 38: training loss 3.9239\n",
      "epoch 39: training loss 3.1584\n",
      "epoch 40: training loss 3.1839\n",
      "epoch 41: training loss 3.2595\n",
      "epoch 42: training loss 3.3886\n",
      "epoch 43: training loss 3.0302\n",
      "epoch 44: training loss 3.2019\n",
      "epoch 45: training loss 3.2217\n",
      "epoch 46: training loss 3.4617\n",
      "epoch 47: training loss 3.4441\n",
      "epoch 48: training loss 3.5302\n",
      "epoch 49: training loss 3.1790\n",
      "epoch 50: training loss 3.3001\n",
      "epoch 51: training loss 3.1574\n",
      "epoch 52: training loss 3.2737\n",
      "epoch 53: training loss 3.2916\n",
      "epoch 54: training loss 3.4299\n",
      "epoch 55: training loss 3.1966\n",
      "epoch 56: training loss 3.3383\n",
      "epoch 57: training loss 3.2920\n",
      "epoch 58: training loss 3.2264\n",
      "epoch 59: training loss 3.1640\n",
      "epoch 60: training loss 3.7135\n",
      "epoch 61: training loss 3.0835\n",
      "epoch 62: training loss 3.2865\n",
      "epoch 63: training loss 2.8772\n",
      "epoch 64: training loss 2.7118\n",
      "epoch 65: training loss 3.6521\n",
      "epoch 66: training loss 3.3831\n",
      "epoch 67: training loss 3.5884\n",
      "epoch 68: training loss 3.6056\n",
      "epoch 69: training loss 3.4740\n",
      "epoch 70: training loss 3.3008\n",
      "epoch 71: training loss 3.1466\n",
      "epoch 72: training loss 3.2098\n",
      "epoch 73: training loss 3.0331\n",
      "epoch 74: training loss 3.3428\n",
      "epoch 75: training loss 3.0625\n",
      "epoch 76: training loss 3.3629\n",
      "epoch 77: training loss 2.9505\n",
      "epoch 78: training loss 3.3362\n",
      "epoch 79: training loss 3.0618\n",
      "epoch 80: training loss 3.1693\n",
      "epoch 81: training loss 3.3508\n",
      "epoch 82: training loss 2.9305\n",
      "epoch 83: training loss 3.1269\n",
      "epoch 84: training loss 3.4933\n",
      "epoch 85: training loss 3.1842\n",
      "epoch 86: training loss 3.1907\n",
      "epoch 87: training loss 3.1577\n",
      "epoch 88: training loss 3.0805\n",
      "epoch 89: training loss 2.6262\n",
      "epoch 90: training loss 3.1108\n",
      "epoch 91: training loss 3.3217\n",
      "epoch 92: training loss 3.2182\n",
      "epoch 93: training loss 3.3497\n",
      "epoch 94: training loss 3.2371\n",
      "epoch 95: training loss 3.0622\n",
      "epoch 96: training loss 3.2807\n",
      "epoch 97: training loss 3.3548\n",
      "epoch 98: training loss 3.1031\n",
      "epoch 99: training loss 3.0851\n",
      "epoch 100: training loss 3.0723\n",
      "epoch 101: training loss 2.9935\n",
      "epoch 102: training loss 2.9875\n",
      "epoch 103: training loss 3.1708\n",
      "epoch 104: training loss 3.2006\n",
      "epoch 105: training loss 3.0275\n",
      "epoch 106: training loss 3.0939\n",
      "epoch 107: training loss 3.0954\n",
      "epoch 108: training loss 2.9228\n",
      "epoch 109: training loss 2.8210\n",
      "epoch 110: training loss 2.9534\n",
      "epoch 111: training loss 2.5774\n",
      "epoch 112: training loss 3.2291\n",
      "epoch 113: training loss 2.8369\n",
      "epoch 114: training loss 3.1644\n",
      "epoch 115: training loss 2.9784\n",
      "epoch 116: training loss 3.0638\n",
      "epoch 117: training loss 3.1197\n",
      "epoch 118: training loss 3.1322\n",
      "epoch 119: training loss 3.2872\n",
      "epoch 120: training loss 2.9174\n",
      "epoch 121: training loss 2.9849\n",
      "epoch 122: training loss 3.1902\n",
      "epoch 123: training loss 2.8642\n",
      "epoch 124: training loss 2.9102\n",
      "epoch 125: training loss 2.8274\n",
      "epoch 126: training loss 2.9426\n",
      "epoch 127: training loss 2.8217\n",
      "epoch 128: training loss 2.9506\n",
      "epoch 129: training loss 3.0667\n",
      "epoch 130: training loss 2.8418\n",
      "epoch 131: training loss 3.0334\n",
      "epoch 132: training loss 3.5437\n",
      "epoch 133: training loss 3.0494\n",
      "epoch 134: training loss 3.0119\n",
      "epoch 135: training loss 2.9817\n",
      "epoch 136: training loss 2.8686\n",
      "epoch 137: training loss 2.8255\n",
      "epoch 138: training loss 2.6448\n",
      "epoch 139: training loss 2.5840\n",
      "epoch 140: training loss 3.0523\n",
      "epoch 141: training loss 3.0167\n",
      "Exceeded the training patience. Terminating the training procedure...\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "# Model training. This is PyPOTS showtime. üí™\n",
    "saits = SAITS(n_steps=50, n_features=6, n_layers=2, d_model=256, d_inner=128, n_head=4, d_k=64, d_v=64, dropout=0.0, epochs=200, patience=30)\n",
    "saits.save_logs_to_tensorboard(saving_path='./runs/saits/', title='test')\n",
    "saits.fit(X)  # train the model. Here I use the whole dataset as the training set, because ground truth is not visible to the model.\n",
    "imputation = saits.impute(X)  # impute the originally-missing values and artificially-missing values\n",
    "mae = cal_mae(imputation, X_intact, indicating_mask)  # calculate mean absolute error on the ground truth (artificially-missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "saits.logger\n",
    "import tensorboard\n",
    "\n",
    "\n",
    "        n_steps,\n",
    "        n_features,\n",
    "        rnn_hidden_size,\n",
    "        learning_rate=1e-3,\n",
    "        epochs=100,\n",
    "        patience=10,\n",
    "        batch_size=32,\n",
    "        weight_decay=1e-5,\n",
    "        device=None,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRITS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No given device, using default device: cuda:0\n",
      "Model initialized successfully. Number of the trainable parameters: 41936\n",
      "epoch 0: training loss 11.1776\n",
      "epoch 1: training loss 10.9682\n",
      "epoch 2: training loss 10.7608\n",
      "epoch 3: training loss 10.5500\n",
      "epoch 4: training loss 10.3391\n",
      "epoch 5: training loss 10.1236\n",
      "epoch 6: training loss 9.9127\n",
      "epoch 7: training loss 9.7042\n",
      "epoch 8: training loss 9.5074\n",
      "epoch 9: training loss 9.3191\n",
      "epoch 10: training loss 9.1319\n",
      "epoch 11: training loss 8.9550\n",
      "epoch 12: training loss 8.7873\n",
      "epoch 13: training loss 8.6238\n",
      "epoch 14: training loss 8.4655\n",
      "epoch 15: training loss 8.3141\n",
      "epoch 16: training loss 8.1671\n",
      "epoch 17: training loss 8.0265\n",
      "epoch 18: training loss 7.8895\n",
      "epoch 19: training loss 7.7559\n",
      "epoch 20: training loss 7.6280\n",
      "epoch 21: training loss 7.5078\n",
      "epoch 22: training loss 7.3886\n",
      "epoch 23: training loss 7.2758\n",
      "epoch 24: training loss 7.1706\n",
      "epoch 25: training loss 7.0697\n",
      "epoch 26: training loss 6.9794\n",
      "epoch 27: training loss 6.8964\n",
      "epoch 28: training loss 6.8164\n",
      "epoch 29: training loss 6.7447\n",
      "epoch 30: training loss 6.6750\n",
      "epoch 31: training loss 6.6059\n",
      "epoch 32: training loss 6.5375\n",
      "epoch 33: training loss 6.4590\n",
      "epoch 34: training loss 6.4830\n",
      "epoch 35: training loss 6.4084\n",
      "epoch 36: training loss 6.3966\n",
      "epoch 37: training loss 6.3196\n",
      "epoch 38: training loss 6.2635\n",
      "epoch 39: training loss 6.2423\n",
      "epoch 40: training loss 6.1911\n",
      "epoch 41: training loss 6.1327\n",
      "epoch 42: training loss 6.1191\n",
      "epoch 43: training loss 6.0293\n",
      "epoch 44: training loss 6.0301\n",
      "epoch 45: training loss 5.9458\n",
      "epoch 46: training loss 5.8957\n",
      "epoch 47: training loss 5.8792\n",
      "epoch 48: training loss 5.7688\n",
      "epoch 49: training loss 5.7712\n",
      "epoch 50: training loss 5.6768\n",
      "epoch 51: training loss 5.6428\n",
      "epoch 52: training loss 5.5444\n",
      "epoch 53: training loss 5.5025\n",
      "epoch 54: training loss 5.6128\n",
      "epoch 55: training loss 5.4307\n",
      "epoch 56: training loss 5.5057\n",
      "epoch 57: training loss 5.4972\n",
      "epoch 58: training loss 5.4204\n",
      "epoch 59: training loss 5.4099\n",
      "epoch 60: training loss 5.3485\n",
      "epoch 61: training loss 5.3172\n",
      "epoch 62: training loss 5.3172\n",
      "epoch 63: training loss 5.2593\n",
      "epoch 64: training loss 5.2729\n",
      "epoch 65: training loss 5.2405\n",
      "epoch 66: training loss 5.2585\n",
      "epoch 67: training loss 5.1882\n",
      "epoch 68: training loss 5.1326\n",
      "epoch 69: training loss 5.1206\n",
      "epoch 70: training loss 5.0514\n",
      "epoch 71: training loss 5.0761\n",
      "epoch 72: training loss 4.9708\n",
      "epoch 73: training loss 5.0072\n",
      "epoch 74: training loss 4.9464\n",
      "epoch 75: training loss 4.9159\n",
      "epoch 76: training loss 4.9064\n",
      "epoch 77: training loss 4.9010\n",
      "epoch 78: training loss 4.8517\n",
      "epoch 79: training loss 4.8164\n",
      "epoch 80: training loss 4.8249\n",
      "epoch 81: training loss 4.8549\n",
      "epoch 82: training loss 4.8385\n",
      "epoch 83: training loss 4.8975\n",
      "epoch 84: training loss 4.7762\n",
      "epoch 85: training loss 4.8757\n",
      "epoch 86: training loss 4.8609\n",
      "epoch 87: training loss 4.7219\n",
      "epoch 88: training loss 4.8207\n",
      "epoch 89: training loss 4.7588\n",
      "epoch 90: training loss 4.6764\n",
      "epoch 91: training loss 4.8070\n",
      "epoch 92: training loss 4.7067\n",
      "epoch 93: training loss 4.7370\n",
      "epoch 94: training loss 4.6924\n",
      "epoch 95: training loss 4.7369\n",
      "epoch 96: training loss 4.6984\n",
      "epoch 97: training loss 4.6310\n",
      "epoch 98: training loss 4.6443\n",
      "epoch 99: training loss 4.5424\n",
      "epoch 100: training loss 4.6191\n",
      "epoch 101: training loss 4.5047\n",
      "epoch 102: training loss 4.5363\n",
      "epoch 103: training loss 4.4834\n",
      "epoch 104: training loss 4.4854\n",
      "epoch 105: training loss 4.4574\n",
      "epoch 106: training loss 4.4429\n",
      "epoch 107: training loss 4.4278\n",
      "epoch 108: training loss 4.4570\n",
      "epoch 109: training loss 4.3893\n",
      "epoch 110: training loss 4.3620\n",
      "epoch 111: training loss 4.3948\n",
      "epoch 112: training loss 4.3625\n",
      "epoch 113: training loss 4.3300\n",
      "epoch 114: training loss 4.2939\n",
      "epoch 115: training loss 4.2812\n",
      "epoch 116: training loss 4.3063\n",
      "epoch 117: training loss 4.2962\n",
      "epoch 118: training loss 4.3117\n",
      "epoch 119: training loss 4.2607\n",
      "epoch 120: training loss 4.2389\n",
      "epoch 121: training loss 4.2477\n",
      "epoch 122: training loss 4.2280\n",
      "epoch 123: training loss 4.1534\n",
      "epoch 124: training loss 4.2803\n",
      "epoch 125: training loss 4.2070\n",
      "epoch 126: training loss 4.1713\n",
      "epoch 127: training loss 4.2013\n",
      "epoch 128: training loss 4.0874\n",
      "epoch 129: training loss 4.1430\n",
      "epoch 130: training loss 4.0710\n",
      "epoch 131: training loss 4.1591\n",
      "epoch 132: training loss 4.0892\n",
      "epoch 133: training loss 4.1087\n",
      "epoch 134: training loss 4.0743\n",
      "epoch 135: training loss 4.0437\n",
      "epoch 136: training loss 4.0517\n",
      "epoch 137: training loss 3.9996\n",
      "epoch 138: training loss 3.9933\n",
      "epoch 139: training loss 3.9844\n",
      "epoch 140: training loss 3.9673\n",
      "epoch 141: training loss 3.9308\n",
      "epoch 142: training loss 3.9418\n",
      "epoch 143: training loss 3.9343\n",
      "epoch 144: training loss 3.8881\n",
      "epoch 145: training loss 3.9028\n",
      "epoch 146: training loss 3.8699\n",
      "epoch 147: training loss 3.8647\n",
      "epoch 148: training loss 3.8568\n",
      "epoch 149: training loss 3.8295\n",
      "epoch 150: training loss 3.8345\n",
      "epoch 151: training loss 3.8096\n",
      "epoch 152: training loss 3.8043\n",
      "epoch 153: training loss 3.8090\n",
      "epoch 154: training loss 3.7533\n",
      "epoch 155: training loss 3.7546\n",
      "epoch 156: training loss 3.7485\n",
      "epoch 157: training loss 3.7262\n",
      "epoch 158: training loss 3.7188\n",
      "epoch 159: training loss 3.6999\n",
      "epoch 160: training loss 3.6611\n",
      "epoch 161: training loss 3.6832\n",
      "epoch 162: training loss 3.6384\n",
      "epoch 163: training loss 3.6076\n",
      "epoch 164: training loss 3.6185\n",
      "epoch 165: training loss 3.5804\n",
      "epoch 166: training loss 3.5489\n",
      "epoch 167: training loss 3.5617\n",
      "epoch 168: training loss 3.5313\n",
      "epoch 169: training loss 3.5244\n",
      "epoch 170: training loss 3.5121\n",
      "epoch 171: training loss 3.4872\n",
      "epoch 172: training loss 3.4768\n",
      "epoch 173: training loss 3.4749\n",
      "epoch 174: training loss 3.4129\n",
      "epoch 175: training loss 3.4507\n",
      "epoch 176: training loss 3.4291\n",
      "epoch 177: training loss 3.3836\n",
      "epoch 178: training loss 3.3757\n",
      "epoch 179: training loss 3.3619\n",
      "epoch 180: training loss 3.3257\n",
      "epoch 181: training loss 3.3071\n",
      "epoch 182: training loss 3.3027\n",
      "epoch 183: training loss 3.2719\n",
      "epoch 184: training loss 3.2842\n",
      "epoch 185: training loss 3.2148\n",
      "epoch 186: training loss 3.2260\n",
      "epoch 187: training loss 3.2152\n",
      "epoch 188: training loss 3.1943\n",
      "epoch 189: training loss 3.1710\n",
      "epoch 190: training loss 3.1599\n",
      "epoch 191: training loss 3.1191\n",
      "epoch 192: training loss 3.1256\n",
      "epoch 193: training loss 3.1518\n",
      "epoch 194: training loss 3.0853\n",
      "epoch 195: training loss 3.1004\n",
      "epoch 196: training loss 3.0616\n",
      "epoch 197: training loss 3.0370\n",
      "epoch 198: training loss 3.0308\n",
      "epoch 199: training loss 3.0300\n",
      "epoch 200: training loss 3.0504\n",
      "epoch 201: training loss 2.9944\n",
      "epoch 202: training loss 3.0746\n",
      "epoch 203: training loss 2.9732\n",
      "epoch 204: training loss 3.0628\n",
      "epoch 205: training loss 2.9765\n",
      "epoch 206: training loss 3.0753\n",
      "epoch 207: training loss 3.0671\n",
      "epoch 208: training loss 3.0016\n",
      "epoch 209: training loss 3.0342\n",
      "epoch 210: training loss 2.9337\n",
      "epoch 211: training loss 2.9870\n",
      "epoch 212: training loss 2.9243\n",
      "epoch 213: training loss 2.9499\n",
      "epoch 214: training loss 2.9372\n",
      "epoch 215: training loss 2.9753\n",
      "epoch 216: training loss 2.9163\n",
      "epoch 217: training loss 2.9824\n",
      "epoch 218: training loss 2.9672\n",
      "epoch 219: training loss 2.8602\n",
      "epoch 220: training loss 2.9319\n",
      "epoch 221: training loss 2.8206\n",
      "epoch 222: training loss 2.8606\n",
      "epoch 223: training loss 2.8197\n",
      "epoch 224: training loss 2.8079\n",
      "epoch 225: training loss 2.7953\n",
      "epoch 226: training loss 2.7947\n",
      "epoch 227: training loss 2.7710\n",
      "epoch 228: training loss 2.7829\n",
      "epoch 229: training loss 2.7658\n",
      "epoch 230: training loss 2.7661\n",
      "epoch 231: training loss 2.7759\n",
      "epoch 232: training loss 2.7417\n",
      "epoch 233: training loss 2.7461\n",
      "epoch 234: training loss 2.7399\n",
      "epoch 235: training loss 2.7332\n",
      "epoch 236: training loss 2.7280\n",
      "epoch 237: training loss 2.7213\n",
      "epoch 238: training loss 2.7252\n",
      "epoch 239: training loss 2.7187\n",
      "epoch 240: training loss 2.7000\n",
      "epoch 241: training loss 2.7198\n",
      "epoch 242: training loss 2.6970\n",
      "epoch 243: training loss 2.6779\n",
      "epoch 244: training loss 2.7031\n",
      "epoch 245: training loss 2.6745\n",
      "epoch 246: training loss 2.6638\n",
      "epoch 247: training loss 2.6734\n",
      "epoch 248: training loss 2.6536\n",
      "epoch 249: training loss 2.6460\n",
      "epoch 250: training loss 2.6624\n",
      "epoch 251: training loss 2.6562\n",
      "epoch 252: training loss 2.6272\n",
      "epoch 253: training loss 2.6086\n",
      "epoch 254: training loss 2.6110\n",
      "epoch 255: training loss 2.6064\n",
      "epoch 256: training loss 2.6244\n",
      "epoch 257: training loss 2.6147\n",
      "epoch 258: training loss 2.5894\n",
      "epoch 259: training loss 2.5769\n",
      "epoch 260: training loss 2.5743\n",
      "epoch 261: training loss 2.5700\n",
      "epoch 262: training loss 2.5748\n",
      "epoch 263: training loss 2.5731\n",
      "epoch 264: training loss 2.5814\n",
      "epoch 265: training loss 2.5602\n",
      "epoch 266: training loss 2.5457\n",
      "epoch 267: training loss 2.5632\n",
      "epoch 268: training loss 2.5345\n",
      "epoch 269: training loss 2.5321\n",
      "epoch 270: training loss 2.5568\n",
      "epoch 271: training loss 2.5411\n",
      "epoch 272: training loss 2.4993\n",
      "epoch 273: training loss 2.5302\n",
      "epoch 274: training loss 2.5324\n",
      "epoch 275: training loss 2.5097\n",
      "epoch 276: training loss 2.4828\n",
      "epoch 277: training loss 2.4889\n",
      "epoch 278: training loss 2.4996\n",
      "epoch 279: training loss 2.5027\n",
      "epoch 280: training loss 2.4885\n",
      "epoch 281: training loss 2.4714\n",
      "epoch 282: training loss 2.4605\n",
      "epoch 283: training loss 2.4618\n",
      "epoch 284: training loss 2.4619\n",
      "epoch 285: training loss 2.4491\n",
      "epoch 286: training loss 2.4392\n",
      "epoch 287: training loss 2.4349\n",
      "epoch 288: training loss 2.4295\n",
      "epoch 289: training loss 2.4323\n",
      "epoch 290: training loss 2.4405\n",
      "epoch 291: training loss 2.4559\n",
      "epoch 292: training loss 2.4560\n",
      "epoch 293: training loss 2.4147\n",
      "epoch 294: training loss 2.4318\n",
      "epoch 295: training loss 2.4585\n",
      "epoch 296: training loss 2.4152\n",
      "epoch 297: training loss 2.3963\n",
      "epoch 298: training loss 2.3973\n",
      "epoch 299: training loss 2.3959\n",
      "epoch 300: training loss 2.3876\n",
      "epoch 301: training loss 2.3852\n",
      "epoch 302: training loss 2.3980\n",
      "epoch 303: training loss 2.3998\n",
      "epoch 304: training loss 2.3847\n",
      "epoch 305: training loss 2.3772\n",
      "epoch 306: training loss 2.3777\n",
      "epoch 307: training loss 2.3931\n",
      "epoch 308: training loss 2.3585\n",
      "epoch 309: training loss 2.3636\n",
      "epoch 310: training loss 2.3700\n",
      "epoch 311: training loss 2.3568\n",
      "epoch 312: training loss 2.3524\n",
      "epoch 313: training loss 2.3568\n",
      "epoch 314: training loss 2.3802\n",
      "epoch 315: training loss 2.3544\n",
      "epoch 316: training loss 2.3537\n",
      "epoch 317: training loss 2.3542\n",
      "epoch 318: training loss 2.3538\n",
      "epoch 319: training loss 2.3330\n",
      "epoch 320: training loss 2.3444\n",
      "epoch 321: training loss 2.3556\n",
      "epoch 322: training loss 2.3157\n",
      "epoch 323: training loss 2.3069\n",
      "epoch 324: training loss 2.3039\n",
      "epoch 325: training loss 2.3167\n",
      "epoch 326: training loss 2.3292\n",
      "epoch 327: training loss 2.3100\n",
      "epoch 328: training loss 2.3016\n",
      "epoch 329: training loss 2.3073\n",
      "epoch 330: training loss 2.3000\n",
      "epoch 331: training loss 2.2966\n",
      "epoch 332: training loss 2.3041\n",
      "epoch 333: training loss 2.2894\n",
      "epoch 334: training loss 2.2848\n",
      "epoch 335: training loss 2.2730\n",
      "epoch 336: training loss 2.2734\n",
      "epoch 337: training loss 2.2864\n",
      "epoch 338: training loss 2.2780\n",
      "epoch 339: training loss 2.2800\n",
      "epoch 340: training loss 2.2951\n",
      "epoch 341: training loss 2.2667\n",
      "epoch 342: training loss 2.2545\n",
      "epoch 343: training loss 2.2789\n",
      "epoch 344: training loss 2.2853\n",
      "epoch 345: training loss 2.2794\n",
      "epoch 346: training loss 2.2480\n",
      "epoch 347: training loss 2.2397\n",
      "epoch 348: training loss 2.2637\n",
      "epoch 349: training loss 2.2533\n",
      "epoch 350: training loss 2.2399\n",
      "epoch 351: training loss 2.2812\n",
      "epoch 352: training loss 2.2457\n",
      "epoch 353: training loss 2.2483\n",
      "epoch 354: training loss 2.2889\n",
      "epoch 355: training loss 2.2406\n",
      "epoch 356: training loss 2.2641\n",
      "epoch 357: training loss 2.2858\n",
      "Exceeded the training patience. Terminating the training procedure...\n",
      "Finished training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pypots.imputation.brits.BRITS at 0x7f0d00ea5d00>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brits = BRITS(n_steps=50, n_features=6, rnn_hidden_size=64, learning_rate=10e-3, epochs=1000, patience=10)\n",
    "brits.save_logs_to_tensorboard(saving_path='./runs/brits/', title='test')\n",
    "brits.fit(X)  # train the model. Here I use the whole dataset as the training set, because ground truth is not visible to the model.\n",
    "# imputation = brits.impute(X)  # impute the originally-missing values and artificially-missing values\n",
    "# mae = cal_mae(imputation, X_intact, indicating_mask)  # calculate mean absolute error on the ground truth (artificially-missing values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('PyPOTS_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c70c2f994a350ab1bc4c43a181023eb6ea79a59ab22a82e0e018a43c74d108f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
