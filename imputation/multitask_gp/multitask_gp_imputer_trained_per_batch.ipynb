{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8e9494c",
   "metadata": {},
   "source": [
    "# Multitask Gaussian Process Imputation\n",
    "\n",
    "This Jupyter notebook contains all the code and examples to for most things concerning Gaussian Process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f32e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import models\n",
    "import utils\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from imputation.hetvae.src.train import HETVAE\n",
    "from toy_dataset import data_utils\n",
    "import utils\n",
    "\n",
    "import math\n",
    "import gpytorch\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcc4865",
   "metadata": {},
   "source": [
    "# Get Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my own synthetic data (Josh)\n",
    "from toy_dataset import data_utils\n",
    "name = 'toydataset_50000'\n",
    "path = data_utils.datasets_dict[name]\n",
    "dataset = data_utils.ToyDataDf(path)\n",
    "dataset.create_mcar_missingness(0.6, -1)\n",
    "dataloader_dict = dataset.prepare_data_mtan(batch_size=128)\n",
    "train_loader = dataloader_dict['train']\n",
    "gt_train_loader = dataloader_dict['train_ground_truth']\n",
    "val_loader = dataloader_dict['validation']\n",
    "gt_validation_loader = dataloader_dict['validation_ground_truth']\n",
    "test_loader = val_loader\n",
    "union_tp = utils.union_time(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13db21f7",
   "metadata": {},
   "source": [
    "---\n",
    "# Define HadamardGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311454fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Hadamard Multitask GP based on \n",
    "# https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/Hadamard_Multitask_GP_Regression.html\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean, MultitaskMean\n",
    "from gpytorch.kernels import Kernel, IndexKernel, MaternKernel, AdditiveKernel\n",
    "from gpytorch.likelihoods import _GaussianLikelihoodBase\n",
    "from gpytorch.likelihoods.noise_models import _HomoskedasticNoiseBase\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.lazy import DiagLazyTensor\n",
    "\n",
    "class HadamardGP(ExactGP):\n",
    "    \"\"\"The base class for a Hadamard multitask GP regression to be used in \n",
    "    conjunction with exact inference.\n",
    "\n",
    "    Args: \n",
    "        num_tasks (int): number of tasks fitted by the model \n",
    "        num_kernels (int, optional): number of kernels to fit; kernels are \n",
    "            combined additively\n",
    "        rank (int, optional): rank of the inter-task correlation\n",
    "    \"\"\"\n",
    "    def __init__(self, train_inputs, train_targets, likelihood, num_tasks, num_kernels=1, rank=1):\n",
    "        super(HadamardGP, self).__init__(train_inputs, train_targets, likelihood)\n",
    "        self.mean_module = HadamardMean(ConstantMean(), num_tasks)\n",
    "        self.covar_module = AdditiveKernel(*[\n",
    "            HadamardKernel(MaternKernel(), num_tasks, rank)\n",
    "            for _ in range(num_kernels)\n",
    "        ])\n",
    "        # print(\"Additive Kernel\", self.covar_module)\n",
    "\n",
    "    def forward(self, input):\n",
    "        mean = self.mean_module(input)\n",
    "        covar = self.covar_module(input)\n",
    "        return MultivariateNormal(mean, covar)\n",
    "\n",
    "\n",
    "class HadamardMean(MultitaskMean):\n",
    "    \"\"\"Mean function for a Hadamard Multitask GP with one learnable constant mean per task\n",
    "\n",
    "    Args:\n",
    "        base_means (:obj:`list` or :obj:`gpytorch.means.Mean`): If a list, each mean is applied to the data.\n",
    "            If a single mean (or a list containing a single mean), that mean is copied `t` times.\n",
    "        num_tasks (int): Number of tasks. If base_means is a list, this should equal its length.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_means, num_tasks):\n",
    "        super(HadamardMean, self).__init__(base_means, num_tasks)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Evaluate the mean in self.base_means corresponding to each element of \n",
    "        the input data, and return as an n-vector of means\n",
    "        \"\"\"\n",
    "        i, x = input[..., [0]], input[..., 1:]\n",
    "        \n",
    "        # Get means at x for each possible task and then gather the right one \n",
    "        # for each row based on the task number i.\n",
    "        means = torch.cat(\n",
    "            [sub_mean(x).unsqueeze(-1) for sub_mean in self.base_means], \n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        # print(means)\n",
    "        # print(means.gather(dim=-1, index=i.long()).squeeze(-1).shape)\n",
    "        # which mean to take based on xi: means(x1, x2, x3, x4) -> means(xi)\n",
    "        return means.gather(dim=-1, index=i.long()).squeeze(-1)\n",
    "\n",
    "\n",
    "class HadamardKernel(Kernel):\n",
    "    \"\"\"Kernel function for a Hadamard Multitask GP of the form \n",
    "\n",
    "    K_x(x_1, x_2) \\times K_i(i_1, i_2)\n",
    "    \n",
    "    where x denotes locations (e.g., in time) and i denotes a task identifier.\n",
    "\n",
    "    Args:\n",
    "        base_kernel (:obj:`gpytorch.kernels.Kernel): the base class for the location kernel K_x, \n",
    "        num_tasks (int): number of tasks denoting size of task covariance K_i\n",
    "        rank (int): rank of the inter-task correlation \n",
    "    \"\"\"\n",
    "    def __init__(self, base_kernel, num_tasks, rank):\n",
    "        super(HadamardKernel, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "        self.base_kernel = base_kernel\n",
    "        \n",
    "        if rank is None:\n",
    "            rank = num_tasks\n",
    "        self.task_covar_module = IndexKernel(num_tasks, rank)\n",
    "\n",
    "    def forward(self, input1, input2, **params):\n",
    "        # print(\"input1:\",input1)\n",
    "        # print(\"input2:\",input2)\n",
    "        i1, x1 = input1[..., 0], input1[..., 1:]\n",
    "        i2, x2 = input2[..., 0], input2[..., 1:]\n",
    "\n",
    "        # Get input-input covariance\n",
    "        covar_x = self.base_kernel(x1, x2, **params)\n",
    "        # print(\"covar_x:\", covar_x.shape)\n",
    "        # Get task-task covariance\n",
    "        covar_i = self.task_covar_module(i1, i2)\n",
    "        # print(\"covar_i:\", covar_i.shape)\n",
    "        # Multiply the two together to get the covariance we want\n",
    "        # print(covar_x.mul(covar_i).shape)\n",
    "        return covar_x.mul(covar_i)\n",
    "\n",
    "\n",
    "class HadamardGaussianLikelihood(_GaussianLikelihoodBase):\n",
    "    r\"\"\"\n",
    "    Likelihood for a Hadamard multitask GP regression. Assumes a different \n",
    "    homoskedastic noise for each task i\n",
    "\n",
    "    p(y_i \\mid f_i) = f_i + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal N (0, \\sigma_i^2)\n",
    "\n",
    "    where :math:`\\sigma_i^2` is the noise parameter of task i.\n",
    "\n",
    "    .. note::\n",
    "        Does not currently allow for batched training. \n",
    "\n",
    "    :param num_tasks: The number of tasks in the multitask GP.\n",
    "    :type num_tasks: int\n",
    "    :param noise_prior: Prior for noise parameter :math:`\\sigma^2`.\n",
    "    :type noise_prior: ~gpytorch.priors.Prior, optional\n",
    "    :param noise_constraint: Constraint for noise parameter :math:`\\sigma^2`.\n",
    "    :type noise_constraint: ~gpytorch.constraints.Interval, optional\n",
    "    \n",
    "    :var torch.Tensor noise: :math:`\\sigma_i^2` parameters (noise)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_tasks, noise_prior=None, noise_constraint=None, **kwargs):\n",
    "        noise_covar = HadamardHomoskedasticNoise(\n",
    "            noise_prior=noise_prior, noise_constraint=noise_constraint, num_tasks=num_tasks\n",
    "        )\n",
    "        super().__init__(noise_covar=noise_covar)\n",
    "\n",
    "    @property\n",
    "    def noise(self):\n",
    "        return self.noise_covar.noise\n",
    "\n",
    "    @noise.setter\n",
    "    def noise(self, value):\n",
    "        self.noise_covar.initialize(noise=value)\n",
    "\n",
    "    @property\n",
    "    def raw_noise(self):\n",
    "        return self.noise_covar.raw_noise\n",
    "\n",
    "    @raw_noise.setter\n",
    "    def raw_noise(self, value):\n",
    "        self.noise_covar.initialize(raw_noise=value)\n",
    "\n",
    "    def __call__(self, input, *args, **kwargs):\n",
    "        if not args:\n",
    "            raise ValueError(\n",
    "                \"The first element of *args must be a list of the training\" \n",
    "                \"inputs.\"\n",
    "            )\n",
    "        \n",
    "        # Extract the task identifiers from the first column of the inputs\n",
    "        # to pass on to the evaluation of self.noise_covar\n",
    "        xi = args[0][0]\n",
    "        i = xi[..., [0]].long()\n",
    "        \n",
    "        # Conditional\n",
    "        if torch.is_tensor(input):\n",
    "            return super().__call__(input, i=i, *args, **kwargs)\n",
    "        # Marginal\n",
    "        elif isinstance(input, MultivariateNormal):\n",
    "            return self.marginal(input, i=i, *args, **kwargs)\n",
    "        # Error\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                \"Likelihoods expects a MultivariateNormal or Normal input to make marginal predictions, or a \"\n",
    "                \"torch.Tensor for conditional predictions. Got a {}\".format(input.__class__.__name__)\n",
    "            )\n",
    "\n",
    "\n",
    "class HadamardHomoskedasticNoise(_HomoskedasticNoiseBase):\n",
    "    r\"\"\"\n",
    "    Noise for a Hadamard multitask GP regression with a different homoskedastic \n",
    "    noise for each task i:\n",
    "    \"\"\"\n",
    "    def __init__(self, noise_prior=None, noise_constraint=None, num_tasks=1):\n",
    "        super().__init__(noise_prior, noise_constraint, torch.Size(), num_tasks)\n",
    "\n",
    "    def forward(self, *params, shape=None, noise=None, i=None, **kwargs):\n",
    "        # Note: removed batching and additional checks/logic for simplicity\n",
    "        \n",
    "        # For each observation, pick the noise indicated by i\n",
    "        noise = self.noise\n",
    "        noise_diag = noise.expand(shape[0], len(noise)).contiguous()\n",
    "        noise_diag = noise_diag.gather(-1, i).squeeze(-1)\n",
    "        return DiagLazyTensor(noise_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b2571e",
   "metadata": {},
   "source": [
    "---\n",
    "# train kernel parameters over entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dfd77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume that batch size is 1\n",
    "# value = values[0]\n",
    "# mask = masks[0]\n",
    "\n",
    "likelihood = HadamardGaussianLikelihood(num_tasks=4)\n",
    "# as a dumy initializer, could be better\n",
    "model = HadamardGP(None, None, likelihood, num_tasks=4, num_kernels=10, rank=4)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "epoch = 10\n",
    "fig_save_folder_path = \"./figs_over_epoch\"\n",
    "os.makedirs(fig_save_folder_path, exist_ok=True)\n",
    "\n",
    "# whether condition the imputer on single task or all tasks\n",
    "condition_on_all_tasks = True\n",
    "\n",
    "for i_epoch in range(epoch):\n",
    "    for i_batch, sample in enumerate(train_loader):\n",
    "        masks = sample[:, :, 4: -1]\n",
    "        values = sample[:, :,:4]\n",
    "        t = sample[0, :, -1]\n",
    "        # go over each sequence within each batch\n",
    "        loss = 0\n",
    "        num_seq = 0\n",
    "        optimizer.zero_grad()\n",
    "        for mask, value in zip(masks, values):\n",
    "            # go over each task within each sequence\n",
    "            train_tasks = []\n",
    "            train_ts = []\n",
    "            train_xs = []\n",
    "            for i in range(4):\n",
    "                value_i = value[:, i]\n",
    "                mask_i = mask[:, i]\n",
    "                # print(value_i.shape)\n",
    "                # print(mask_i.shape)\n",
    "                train_ts.append(t[mask_i==1])\n",
    "                train_xs.append(value_i[mask_i==1])\n",
    "                # print(i, train_ts[-1].shape)\n",
    "\n",
    "                train_task = torch.full((train_ts[-1].shape[0],1), dtype=torch.long, fill_value=i)\n",
    "                train_tasks.append(train_task)\n",
    "\n",
    "            full_train_t = torch.cat(train_ts)\n",
    "            full_train_tasks = torch.cat(train_tasks)\n",
    "            full_train_x = torch.cat(train_xs)\n",
    "            num_seq += 1\n",
    "\n",
    "\n",
    "            input_task_t = torch.cat((full_train_tasks, full_train_t.view(full_train_t.shape[0], 1)), dim=-1)\n",
    "            target_x = full_train_x\n",
    "\n",
    "            model.set_train_data(input_task_t, target_x, strict=False)\n",
    "            output = model(input_task_t)\n",
    "            loss += -mll(output, target_x, [input_task_t])\n",
    "            # global_seq_counter += 1\n",
    "\n",
    "        loss /= num_seq\n",
    "        loss.backward()\n",
    "        print('Iter %dth batch - Loss: %.3f' % (i_batch + 1, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "        # validation step\n",
    "        if i_batch % 50 == 0:\n",
    "            sample_val = next(iter(val_loader))\n",
    "            masks_val = sample_val[:, :, 4: -1]\n",
    "            values_val = sample_val[:, :,:4]\n",
    "            t_val = sample_val[0, :, -1]\n",
    "\n",
    "            gt_sample_val = next(iter(gt_validation_loader))\n",
    "            gt_masks_val = gt_sample_val[:, :, 4: -1]\n",
    "            gt_values_val = gt_sample_val[:, :,:4]\n",
    "            gt_t_val = gt_sample_val[0, :, -1]\n",
    "\n",
    "\n",
    "            value_val = values_val[0]\n",
    "            mask_val = masks_val[0]\n",
    "            \n",
    "            ### condition the model on all context data\n",
    "            if condition_on_all_tasks:\n",
    "                val_tasks_context = []\n",
    "                val_ts_context = []\n",
    "                val_xs_context = []\n",
    "                for i in range(4):\n",
    "                    value_i_context = values_val[0][:, i]\n",
    "                    mask_i_context = masks_val[0][:, i]\n",
    "                    val_ts_context.append(t_val[mask_i_context==1])\n",
    "                    val_xs_context.append(value_i_context[mask_i_context==1])\n",
    "                    val_task_context = torch.full((val_ts_context[-1].shape[0],1), dtype=torch.long, fill_value=i)\n",
    "                    val_tasks_context.append(val_task_context)\n",
    "                full_val_t_context = torch.cat(val_ts_context)\n",
    "                full_val_tasks_context = torch.cat(val_tasks_context)\n",
    "                full_val_x_context = torch.cat(val_xs_context)\n",
    "                input_task_t_context = torch.cat((full_val_tasks_context, full_val_t_context.view(full_val_t_context.shape[0], 1)), dim=-1)\n",
    "                model.set_train_data(input_task_t_context, full_val_x_context, strict=False)\n",
    "\n",
    "            \n",
    "            f, axes = plt.subplots(4, 1, figsize=(10, 28))\n",
    "            titles = ['Noise', 'Trend', \"Seasonality\", \"Trend + Seasonality\"]\n",
    "            \n",
    "            for i in range(4):\n",
    "                \n",
    "                ### condition the model on one task\n",
    "                value_i_context = value_val[:, i]\n",
    "                mask_i_context = mask_val[:, i]\n",
    "\n",
    "                val_t_context = t_val[mask_i_context==1]\n",
    "                val_x_context = value_i_context[mask_i_context==1]\n",
    "\n",
    "\n",
    "                val_task_context = torch.full((val_t_context.shape[0],1), dtype=torch.long, fill_value=i)\n",
    "                val_task_target = torch.full((t_val.shape[0],1), dtype=torch.long, fill_value=i)\n",
    "\n",
    "\n",
    "                input_task_t_context = torch.cat((val_task_context, val_t_context.view(val_t_context.shape[0], 1)), dim=-1)\n",
    "                input_task_t_target = torch.cat((val_task_target, t_val.view(t_val.shape[0], 1)), dim=-1)\n",
    "                if not condition_on_all_tasks:\n",
    "                    model.set_train_data(input_task_t_context, val_x_context, strict=False)\n",
    "                \n",
    "                with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "                    # output_val = model(input_task_t)\n",
    "                    model.eval() # necessary if you want to plot lower and upper bound!!!\n",
    "                    observed_pred = likelihood(model(input_task_t_target), [input_task_t_target])\n",
    "                    # loss_val = -mll(output_val, val_x, [input_task_t])\n",
    "                    # print(f\"val loss: {loss_val}\")\n",
    "                    # Plot training data as black stars\n",
    "                    axes[i].plot(val_t_context.detach().numpy(), val_x_context.detach().numpy(), 'k*')\n",
    "                    lower_2d, upper_2d = observed_pred.confidence_region()\n",
    "                    std = (upper_2d - observed_pred.mean) / 2.0\n",
    "                    upper_1d = observed_pred.mean + std\n",
    "                    lower_1d = observed_pred.mean - std\n",
    "                    \n",
    "                    \n",
    "                    # Predictive mean as blue line\n",
    "                    axes[i].plot(t_val.detach().numpy(), observed_pred.mean.detach().numpy(), 'b')\n",
    "                    # Shade in confidence\n",
    "                    axes[i].fill_between(t_val.detach().numpy(), lower_1d.detach().numpy(), upper_1d.detach().numpy(), alpha=0.5)\n",
    "                    # ax.set_ylim([-3, 3])\n",
    "                    axes[i].plot(t_val.detach().numpy(), gt_values_val[0,:,i].detach().numpy(), 'r')\n",
    "                    axes[i].legend(['Observed Data', 'Mean', 'Confidence', 'ground truth'])\n",
    "                    axes[i].set_title(titles[i])\n",
    "                    model.train()\n",
    "                    for _ in range(100):\n",
    "                        sampled_seq = observed_pred.sample()\n",
    "                        axes[i].plot(t_val.detach().numpy(), sampled_seq.detach().numpy(), 'g', alpha=0.1)\n",
    "                        \n",
    "            print(f\"subplots for {i_epoch}th epoch {i_batch}th batch: \")\n",
    "            # plt.show()\n",
    "            fig_path = os.path.join(fig_save_folder_path, f\"plots_{i_epoch}e_{i_batch}b.png\")\n",
    "            f.savefig(fig_path)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf9dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
