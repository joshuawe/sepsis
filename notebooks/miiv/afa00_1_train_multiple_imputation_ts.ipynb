{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447effeb-1650-4f29-93ad-09b100417273",
   "metadata": {},
   "source": [
    "# (0.1) Train multiple-imputation (MI) models\n",
    "One option to resolve missingness for AFA is to use Multiple Imputation. \n",
    "This notebook trains MI models and saves the multiple imputed datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73390d53-00db-403b-b474-735a5febcf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2d091-d5fb-4212-b7fd-f94cfb57f50e",
   "metadata": {},
   "source": [
    "## Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b60b58e3-6056-41fe-b9a2-dd7e2d28bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from afa.configurations.utils_ts import specify_default_paths_ts\n",
    "# which dataset to work on \n",
    "dataset_name   = \"miiv\"\n",
    "\n",
    "# name for of missingness scenario \n",
    "miss_scenario  = 'fully_observed'\n",
    "\n",
    "# automatically specify some path locations (change paths manually if needed) \n",
    "paths = specify_default_paths_ts(dataset_name = dataset_name , miss_scenario = miss_scenario) \n",
    "\n",
    "# name for ps_model \n",
    "mi_model_name  = 'gaussian_process'\n",
    "\n",
    "# new (where to save the model) \n",
    "mi_model_dir = paths['data_dir']  + 'mi_models' + '/' + mi_model_name + '/'\n",
    "if miss_scenario == 'fully_observed':\n",
    "    paths['data_file']          = '/home2/joshua.wendland/Documents/data/ts/miiv/fully_observed/miiv_static.parquet'\n",
    "    paths['temporal_data_file'] = '/home2/joshua.wendland/Documents/data/ts/miiv/fully_observed/miiv_ts_wide.parquet'\n",
    "    paths['miss_model_files'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49defcd-c9c2-466e-b156-63275bb006c8",
   "metadata": {},
   "source": [
    "### Define model specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dd5657c-ee90-4f83-921a-557ce99c03e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'  # 'cuda' or 'cpu'\n",
    "\n",
    "# Config for dataset preparation (torch.DataSet class)\n",
    "dataset_params = {\n",
    "    'missingness_value': 'nan',   # int, float or 'nan'\n",
    "    'missingness_rate': (0.1, 0.3),\n",
    "    'device': device  # 'cuda' or 'cpu'\n",
    "}\n",
    "\n",
    "# Config for dataloader (torch.DataLoader class)\n",
    "dataloader_params = {\n",
    "    'batch_size': 50, \n",
    "    'shuffle': False, \n",
    "    # 'prefetch_factor': 1, # increase for speed up\n",
    "    # 'num_workers': 0,     # set higher for faster throughput\n",
    "    'drop_last': True\n",
    "}\n",
    "\n",
    "# Config for trainer (pytorch_lightning.Trainer class)\n",
    "trainer_params = {\n",
    "    'max_epochs': 10,    # number of epochs to train\n",
    "    'auto_lr_find': False,\n",
    "    'fast_dev_run': True,\n",
    "    'accelerator': device,\n",
    "    'devices': 1,\n",
    "    'profiler': None,  # 'simple', 'advanced', None\n",
    "    'num_sanity_val_steps': 1\n",
    "}\n",
    "\n",
    "# Config for gp_model (GPImputer class)\n",
    "gp_params = {\n",
    "    'model_type': 'gaussian_process',\n",
    "    'dataset_params' : dataset_params,\n",
    "    'dataloader_params' : dataloader_params,\n",
    "    'trainer_params' : trainer_params,\n",
    "    'num_tasks': 49,  # number of tasks == number of features\n",
    "    'num_kernels': 10,\n",
    "    'data_mode': 'no_simulation',   # 'no_simulation' or 'simulation', with simulation a ground truth is expected to passed as well, ground truth = values for data that is missing in train dataloader\n",
    "    # 'ckpt_path': 'best_model-v_recon_loss_target=1.10-epoch=142.ckpt',  # path to checkpoint of trained model, full path or relative to model directory    \n",
    "}\n",
    "    \n",
    "# Config for mi_model from AFA module (MultipleImputationModel_ts class)\n",
    "mi_model_params = {\n",
    "    'name' : mi_model_name, \n",
    "    'directory' : mi_model_dir,\n",
    "    'base_model_params' : gp_params\n",
    "}\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a64631a-d0c5-437d-82f9-2427a44f2b27",
   "metadata": {},
   "source": [
    "## Load dataset with missingness \n",
    "At first, we want to load the dataset \n",
    "\n",
    "Includes loading: \n",
    "- superfeature mapping\n",
    "- problem\n",
    "- afa_problem \n",
    "- missingness_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a59d767-871f-4f22-b635-f2af3e832341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 12:10:26.501523: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-05 12:10:26.593242: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-05 12:10:26.593261: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-05 12:10:33.884767: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-05 12:10:33.884848: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-05 12:10:33.884856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from afa.data_modelling.datasets.data_loader.data_loader_ts import DataLoader_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7807d31-a689-4abd-a656-7c4f1f147506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding sequences: 100%|██████████| 67056/67056 [00:26<00:00, 2539.73it/s]\n",
      "Padding sequences: 100%|██████████| 67056/67056 [00:32<00:00, 2061.38it/s]\n",
      "Padding sequences: 100%|██████████| 67056/67056 [00:27<00:00, 2463.17it/s]\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader_ts(     data_file                  = paths['data_file'],\n",
    "                                 temporal_data_file         = paths['temporal_data_file'],\n",
    "                                 superfeature_mapping_file  = paths['superfeature_mapping_file'],\n",
    "                                 problem_file               = paths['problem_file'],\n",
    "                                 afa_problem_files          = paths['afa_problem_files'], \n",
    "                                 miss_model_files           = paths['miss_model_files'], \n",
    "                                 folds_file                 = paths['folds_file'] )\n",
    "dataset = data_loader.load(temporal_is_wide=True) \n",
    "if miss_scenario == 'fully_observed':\n",
    "    dataset.miss_model = type('Dummy', (object,), {})\n",
    "    dataset.miss_model.m_graph = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82c2bee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataset.get_data(fold=None, split='train')['temporal_feature'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffd0f4-07fd-4e6d-a2cb-74ea1eea2c58",
   "metadata": {},
   "source": [
    "## Define MI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af012bf8-da5c-4699-aec1-f4d5e74af718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_name': 'toydataset_50000', 'data_missingness': 0.6, 'num_kernels': 10, 'num_tasks': 49, 'rank': 4, 'data_mode': 'no_simulation', 'lr': 0.01, 'batch_size': 128, 'sample_tp': 0.4, 'sample_tp_interval': [0.3, 0.8], 'num_epochs': 10, 'model_weights_save_path': './model_weights', 'model_type': 'gaussian_process', 'dataset_params': {'missingness_value': 'nan', 'missingness_rate': (0.1, 0.3), 'device': 'cuda'}, 'dataloader_params': {'batch_size': 50, 'shuffle': False, 'drop_last': True}, 'trainer_params': {'max_epochs': 10, 'auto_lr_find': False, 'fast_dev_run': True, 'accelerator': 'cuda', 'devices': 1, 'profiler': None, 'num_sanity_val_steps': 1}, 'directory': '../../../data/ts/miiv/fully_observed/mi_models/gaussian_process/', 'mode': 'imputation', 'task_names': ['Noise', 'Trend', 'Seasonality', 'Trend + Seasonality']} \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    }
   ],
   "source": [
    "from afa.data_modelling.missingness.multiple_imputation.multiple_imputation_model_ts import MultipleImputationModel_ts\n",
    "\n",
    "mi_model = MultipleImputationModel_ts(  name                         = mi_model_params['name'], \n",
    "                                        m_graph                      = dataset.miss_model.m_graph, \n",
    "                                        superfeature_mapping         = dataset.superfeature_mapping,\n",
    "                                        target_superfeature_names    = dataset.afa_problem.target_superfeature_names,\n",
    "                                        model_params                 = mi_model_params,\n",
    "                                        directory                    = mi_model_params['directory'] ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eaaaf5-3403-46c3-8bf9-7dffd270853b",
   "metadata": {},
   "source": [
    "## Train MI model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cee7ae1-f3e9-4375-9551-ce7921214634",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mi_model\u001b[39m.\u001b[39;49mfit(dataset, fold \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m, train_split \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, valid_split \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m'\u001b[39;49m, fit_again \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Documents/afa_ts/afa/data_modelling/missingness/multiple_imputation/multiple_imputation_model_ts.py:65\u001b[0m, in \u001b[0;36mMultipleImputationModel_ts.fit\u001b[0;34m(self, dataset, fold, train_split, valid_split, fit_again)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mFit the multiple imputation model and plot the outcome of the fitting process. \u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39mSets 'fig_training'. \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m    whether to fit again (if already trained)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39m# fit on training split\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit( dataset \u001b[39m=\u001b[39;49m dataset, \n\u001b[1;32m     66\u001b[0m            train_fold \u001b[39m=\u001b[39;49m fold, \n\u001b[1;32m     67\u001b[0m            train_split \u001b[39m=\u001b[39;49m train_split,\n\u001b[1;32m     68\u001b[0m            val_fold\u001b[39m=\u001b[39;49m fold,\n\u001b[1;32m     69\u001b[0m            val_split\u001b[39m=\u001b[39;49m valid_split, \n\u001b[1;32m     70\u001b[0m            fit_again \u001b[39m=\u001b[39;49m fit_again)\n\u001b[1;32m     72\u001b[0m \u001b[39m# validation set\u001b[39;00m\n\u001b[1;32m     73\u001b[0m data_valid       \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mget_data(fold \u001b[39m=\u001b[39m fold, split \u001b[39m=\u001b[39m valid_split)\n",
      "File \u001b[0;32m~/Documents/afa_ts/afa/data_modelling/missingness/multiple_imputation/multiple_imputation_model_ts.py:241\u001b[0m, in \u001b[0;36mMultipleImputationModel_ts._fit\u001b[0;34m(self, dataset, train_fold, train_split, val_fold, val_split, fit_again)\u001b[0m\n\u001b[1;32m    239\u001b[0m val_data \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mget_data(fold \u001b[39m=\u001b[39m val_fold, split \u001b[39m=\u001b[39m val_split)\n\u001b[1;32m    240\u001b[0m Xy_val, t_context_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess(val_data)\n\u001b[0;32m--> 241\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49mfit(X_train\u001b[39m=\u001b[39;49mXy_train, \n\u001b[1;32m    242\u001b[0m                     t_train\u001b[39m=\u001b[39;49mt_context_train, \n\u001b[1;32m    243\u001b[0m                     X_val\u001b[39m=\u001b[39;49mXy_val, \n\u001b[1;32m    244\u001b[0m                     t_val\u001b[39m=\u001b[39;49mt_context_val,  \n\u001b[1;32m    245\u001b[0m                     fit_again \u001b[39m=\u001b[39;49m fit_again\n\u001b[1;32m    246\u001b[0m ) \n\u001b[1;32m    247\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/afa_ts/afa/data_modelling/models/temporal_models/mi_models/gp_model.py:126\u001b[0m, in \u001b[0;36mGaussianProcessImputer.fit\u001b[0;34m(self, X_train, t_train, X_val, t_val, fit_again, model_version)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, \n\u001b[1;32m    119\u001b[0m         X_train: torch\u001b[39m.\u001b[39mTensor, \n\u001b[1;32m    120\u001b[0m         t_train: torch\u001b[39m.\u001b[39mTensor, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m         model_version: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m): \n\u001b[1;32m    125\u001b[0m     \u001b[39m# preprocess: concatenate Xy and t, bring to correct device\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     Xt_train \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess_X_and_t(X_train, t_train)\n\u001b[1;32m    127\u001b[0m     \u001b[39m# get train dataloader\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     dataloader_mcar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_dataloader(Xt_train, missingness_rate\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpars)   \u001b[39m# dataloader with induced MCAR missingness -> train loader\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/afa_ts/afa/data_modelling/models/temporal_models/mi_models/gp_model.py:207\u001b[0m, in \u001b[0;36mGaussianProcessImputer.preprocess_X_and_t\u001b[0;34m(self, X, t)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39m# check for corrct input types\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(X, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m--> 207\u001b[0m     X \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(X)\n\u001b[1;32m    208\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    209\u001b[0m     t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(t)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "mi_model.fit(dataset, fold = 0, train_split = 'train', valid_split = 'val', fit_again = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a323f-b345-4a74-90aa-5ea170b0451e",
   "metadata": {},
   "source": [
    "## Create multiple imputed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc5fbc-6787-45f6-8832-0e7a9cebad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from afa.data_modelling.missingness.multiple_imputation.multiple_imputed_dataset_ts import MultipleImputedDataset_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afc7a63-5409-4327-b48e-829d285411c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_results = mi_model.predict(dataset, n_samples = 5)\n",
    "\n",
    "# create an mi_dataset out of the generated imputations\n",
    "mi_dataset = MultipleImputedDataset_ts(  dataset = dataset, model = mi_model, results = mi_results) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c90c49-db33-426f-81c0-d53e70986225",
   "metadata": {},
   "source": [
    "## Evaluate imputation model on ground truth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7c58a8-7b09-4174-8e78-1c3be89a5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c9f03-5639-4f5f-baef-36cbea96789b",
   "metadata": {},
   "source": [
    "## Save MI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37204a6c-b06f-4ff0-8559-f6977ecf1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_dataset.save( model_dir = mi_model_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb32c7-1f0b-4f16-9fd4-987260ffed06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef49f9-dbde-4665-9ad8-c3eec083b86a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
