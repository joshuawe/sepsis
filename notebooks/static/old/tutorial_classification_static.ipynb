{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ded17c-9cfc-4874-b947-c32337a2c6cb",
   "metadata": {},
   "source": [
    "# Test data loading for static dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109eebe4-2c2c-441b-bae1-b9167cde31ef",
   "metadata": {},
   "source": [
    "This is a short test to show how data loading works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24951169-b71d-4937-a635-5a1e924d82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# all paths relative to afa directory \n",
    "sys.path.insert(0, os.path.abspath('../../afa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4d90f6-fe03-41f3-8eac-7718598f58cc",
   "metadata": {},
   "source": [
    "### Load normal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2010739-c38b-4238-a13c-a691b0b06547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from afa.data_modelling.datasets.data_loader.data_loader_static import DataLoader_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a598febe-3424-49af-8a4c-e2b1e8d0b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../../data/static/synthetic_1/'\n",
    "data_file = data_dir + 'synthetic_1.csv' \n",
    "superfeature_mapping_file = data_dir + 'superfeatures.csv'\n",
    "# superfeature_mapping_file = None\n",
    "    \n",
    "data_loader = DataLoader_static( data_file     = data_file,\n",
    "                                 superfeature_mapping_file  = superfeature_mapping_file )\n",
    "\n",
    "dataset = data_loader.load() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe671d7-5be7-4958-894a-d9ee1ed52278",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aa39498-1b20-4fbe-a2d2-e3429e3960c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-28 12:12:30.598077: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-28 12:12:30.719413: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-28 12:12:30.719430: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-28 12:12:31.496054: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-28 12:12:31.496163: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-28 12:12:31.496173: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from afa.data_modelling.preprocessing import Normalizer\n",
    "from afa.data_modelling.utils.model_utils import PipelineComposer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "492bb2f5-8d21-46a8-8464-378a1635c949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish preprocessing.\n"
     ]
    }
   ],
   "source": [
    "normalization = 'minmax'\n",
    "normalizer = Normalizer(normalization)\n",
    "\n",
    "# Data preprocessing\n",
    "filter_pipeline = PipelineComposer(normalizer)\n",
    "\n",
    "dataset = filter_pipeline.fit_transform(dataset)\n",
    "\n",
    "print('Finish preprocessing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2584bb5-bacd-40a0-a671-75da0e38fae6",
   "metadata": {},
   "source": [
    "### Define problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b190a71-326b-46c8-bdfd-1c209dc4b0ff",
   "metadata": {},
   "source": [
    "Set the prediction problem that we want to solve. Set the problem (one-shot), set the label and set the treatment features (not supported). \n",
    "\n",
    "We also define the metric for evaluation and the task itself (whether classification or regression\n",
    "  - problem: \n",
    "    - 'classification'\n",
    "  - label_name: the column name for the label(s)\n",
    "  - treatment: the column name for treatments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30f46286-3d6f-4cec-b9e0-b9c20d6f3144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from afa.data_modelling.problem.problem_static import ProblemMaker_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6798e31d-879a-4c08-8772-a83a515b8e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = 'Y'\n",
    "problem = 'classification'\n",
    "treatment = None\n",
    "\n",
    "# Define problem \n",
    "problem_maker = ProblemMaker_static(problem=problem, label=[label_name], treatment=treatment)\n",
    "\n",
    "dataset  = problem_maker.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126274c9-d2e2-4fa3-b449-9d9932dbf95e",
   "metadata": {},
   "source": [
    "## Impute dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4701397d-7db3-468d-9838-b5f405b94ac5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_modelling.imputation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_modelling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimputation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Imputation\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_modelling.imputation'"
     ]
    }
   ],
   "source": [
    "from afa.data_modelling.imputation import Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48d284c-4983-4df5-b828-139f2914a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set imputation models\n",
    "imputation_model = 'median'\n",
    "\n",
    "# Impute the missing data\n",
    "imputation          = Imputation(imputation_model_name = imputation_model)\n",
    "\n",
    "imputation_pipeline = PipelineComposer(imputation)\n",
    "\n",
    "dataset_train = imputation_pipeline.fit_transform(dataset_train)\n",
    "dataset_test  = imputation_pipeline.transform(dataset_test)\n",
    "\n",
    "print('Finish imputation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85352148-bd7f-479c-98e1-26654e283231",
   "metadata": {},
   "source": [
    "## Perform prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fd8a99-830d-438e-8762-73d12615d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the predictive model\n",
    "task = 'classification'\n",
    "pred_class = prediction_miss(   classification_params       = configs['classifier']['class_params'], \n",
    "                                task          = task,\n",
    "                                imp_params    = configs['classifier']['imp_params'],  \n",
    "                                miss_params   = configs['classifier']['miss_params'])\n",
    "pred_class.fit(dataset_train, \n",
    "               fold=0, \n",
    "               train_split=configs['data_splits']['models']['classifier_train'], \n",
    "               valid_split=configs['data_splits']['models']['classifier_test'])\n",
    "\n",
    "# performance on given dataset:\n",
    "Y_tilde_test     = pred_class.predict(dataset_train, fold=0, \n",
    "                                      test_split= configs['data_splits']['models']['classifier_test'])\n",
    "_, Y_test , _    = dataset_train.get_fold(fold=0, \n",
    "                                          split = configs['data_splits']['models']['classifier_test']) \n",
    "\n",
    "accuracy = accuracy_score( Y_tilde_test.round() , Y_test.round())\n",
    "print(\"Accuracy on testset: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7001791-5e55-4a97-955d-c6fd02349367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afa_env",
   "language": "python",
   "name": "afa_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
